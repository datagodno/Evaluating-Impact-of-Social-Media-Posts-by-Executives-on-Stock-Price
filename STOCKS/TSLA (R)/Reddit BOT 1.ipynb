{"cells":[{"cell_type":"markdown","metadata":{"id":"8FG-mIuVW1cP"},"source":["# Reddit Trading Bot"]},{"cell_type":"markdown","metadata":{"id":"XC57F93BW1ee"},"source":["# To get all posts from a subreddit"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-01-18T14:02:15.261190Z","start_time":"2022-01-18T14:02:15.251189Z"},"id":"eGkwO7_AW1eg"},"outputs":[],"source":["import requests\n","from datetime import datetime\n","import traceback\n","import time\n","import json\n","import sys\n","import re\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-01-18T14:36:05.389768Z","start_time":"2022-01-18T14:36:05.370755Z"},"id":"0XD1MnD4W1em"},"outputs":[],"source":["df = pd.DataFrame(columns = ['Score', 'date', 'text', 'subreddit'])\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-01-18T15:14:36.826792Z","start_time":"2022-01-18T15:14:36.816780Z"},"id":"4zx1tu68W1ex"},"outputs":[],"source":["username = \"\"  # put the username you want to download in the quotes\n","subreddits_general =['tsla',\n","            'teslainvestorsclub',\n","            'TSLALounge',\n","            'TSLAtalk',\n","            'TSLAsexy',\n","            'Tesla_Stock',\n","            'tslaq']\n","subreddits_exec = [\n","                   'stocks',\n","                   'investing',\n","                   'pennystocks',\n","                   'robinhood',\n","                   'investing_discussion',\n","                   'cryptocurrency',\n","                   'Dividends',\n","                   'Dividends ',\n","                    'wallstreetbets'\n","\n","                   ]\n","# From https://www.investopedia.com/reddit-top-investing-and-trading-communities-5189322\n","# put the subreddit you want to download in the quotes\n","# leave either one blank to download an entire user's or subreddit's history\n","# or fill in both to download a specific users history from a specific subreddit\n","\n","url = \"https://api.pushshift.io/reddit/{}/search?limit=1000&sort=desc&{}&before=\"\n","\n","start_time = datetime.utcnow()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-01-18T15:14:37.518835Z","start_time":"2022-01-18T15:14:37.492833Z"},"id":"bMZP1wBxW1e4"},"outputs":[],"source":["def downloadFromUrl(object_type, fs):\n","    global df\n","    #print(f\"Saving {object_type}s to {filename}\")\n","    filter_string = fs\n","    print(fs,\" *********----------------started--------------***************\")\n","    count = 0\n","    #handle = open(filename, 'w')\n","    previous_epoch = int(start_time.timestamp())\n","    while True:\n","        new_url = url.format(object_type, filter_string)+str(previous_epoch)\n","        json_text = requests.get(new_url, headers={'User-Agent': \"Post downloader by /u/Watchful1\"})\n","        time.sleep(1)  # pushshift has a rate limit, if we send requests too fast it will start returning error messages\n","        try:\n","            json_data = json_text.json()\n","        except json.decoder.JSONDecodeError:\n","            time.sleep(1)\n","            continue\n","\n","        if 'data' not in json_data:\n","            break\n","        objects = json_data['data']\n","        if len(objects) == 0:\n","            break\n","\n","        for object in objects:\n","            previous_epoch = object['created_utc'] - 1\n","            \n","            if object_type == 'comment':\n","                try:\n","                    '''handle.write(str(object['score']))\n","                    handle.write(\" : \")\n","                    handle.write(datetime.fromtimestamp(object['created_utc']).strftime(\"%Y-%m-%d\"))\n","                    handle.write(\"\\n\")\n","                    handle.write(object['body'].encode(encoding='ascii', errors='ignore').decode())\n","                    handle.write(\"\\n-------------------------------\\n\")'''\n","                    tweet  = object['body'].encode(encoding='ascii', errors='ignore').decode()\n","                    if tweet.find(\"TSLA\") >= 0 or tweet.find(\"TESLA\") >= 0 or tweet.find(\"Tesla\") >= 0 or tweet.find(\"tesla\") >= 0 or tweet.find(\"tsla\") >= 0 :\n","                        if object['score'] >= 0:\n","                            df = df.append({'Score' : object['score'],\n","                                            'date' : datetime.fromtimestamp(object['created_utc']).strftime(\"%Y-%m-%d\"),\n","                                            'text' : tweet,\n","                                            'subreddit':filter_string},\n","                                           ignore_index = True)\n","                            count += 1\n","                except Exception as err:\n","                    print(f\"Couldn't print comment: https://www.reddit.com{object['permalink']}\")\n","                    print(traceback.format_exc())\n","            elif object_type == 'submission':\n","                if object['is_self']:\n","                    if 'selftext' not in object:\n","                        continue\n","                    try:\n","                        '''handle.write(str(object['score']))\n","                        handle.write(\" : \")\n","                        handle.write(datetime.fromtimestamp(object['created_utc']).strftime(\"%Y-%m-%d\"))\n","                        handle.write(\"\\n\")\n","                        handle.write(object['selftext'].encode(encoding='ascii', errors='ignore').decode())\n","                        handle.write(\"\\n-------------------------------\\n\")'''\n","                        tweet = object['selftext'].encode(encoding='ascii', errors='ignore').decode()\n","                        \n","                        if tweet.find(\"TSLA\") >= 0 or tweet.find(\"TESLA\") >= 0 or tweet.find(\"Tesla\") >= 0 or tweet.find(\"tesla\") >= 0 or tweet.find(\"tsla\") >= 0 :\n","                            if object['score'] >= 1:\n","                                df = df.append({'Score' : object['score'],\n","                                                'date' : datetime.fromtimestamp(object['created_utc']).strftime(\"%Y-%m-%d\"),\n","                                                'text' : tweet,\n","                                                'subreddit':filter_string},\n","                                               ignore_index = True)\n","                                count += 1\n","                    except Exception as err:\n","                        print(f\"Couldn't print post: {object['url']}\")\n","                        print(traceback.format_exc())\n","\n","        print(\"Saved {} {}s through {} for {}\".format(count, object_type, datetime.fromtimestamp(previous_epoch).strftime(\"%Y-%m-%d\"), fs))\n","\n","    print(f\"Saved {count} {object_type}s\")\n","\n","    #handle.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-01-18T16:56:43.127246Z","start_time":"2022-01-18T15:14:38.279665Z"},"scrolled":true,"id":"sxqFdrPeW1e-"},"outputs":[],"source":["for subreddit in subreddits_general:\n","    filter_string = None\n","    if username == \"\" and subreddit == \"\":\n","        print(\"Fill in either username or subreddit\")\n","        sys.exit(0)\n","    elif username == \"\" and subreddit != \"\":\n","        filter_string = f\"subreddit={subreddit}\"\n","    elif username != \"\" and subreddit == \"\":\n","        filter_string = f\"author={username}\"\n","    else:\n","        filter_string = f\"author={username}&subreddit={subreddit}\"\n","        \n","    downloadFromUrl(\"submission\", filter_string)\n","    print(\" \"+subreddit+\" ********------------------COMPLETED----------********************************\")\n","   \n","    time.sleep(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bDdtEHdaW1fC"},"outputs":[],"source":["df.columns =['upvotes','Date','Post', 'sub']\n","subs =[]\n","for line in df['sub']:\n","  line = re.sub('subreddit=', '', line)\n","  subs.append(line)\n","df['sub'] = subs\n","df"]},{"cell_type":"code","source":["df['sub'].value_counts()"],"metadata":{"id":"bhYkH2DClwSK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.drop_duplicates(subset =[\"Post\"],\n","                     keep = 'first', inplace = True)\n","df"],"metadata":{"id":"ftwelQUjsm6Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NDeVYGSbW1fH"},"outputs":[],"source":["df.to_csv('GENERALRedditTesla.csv', sep=',', index = False)\n","print('csv done')"]},{"cell_type":"code","source":["df = pd.DataFrame(columns = ['Score', 'date', 'text', 'subreddit'])\n","df"],"metadata":{"id":"LkjO_2zys_83"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for subreddit in subreddits_exec:\n","    filter_string = None\n","    if username == \"\" and subreddit == \"\":\n","        print(\"Fill in either username or subreddit\")\n","        sys.exit(0)\n","    elif username == \"\" and subreddit != \"\":\n","        filter_string = f\"subreddit={subreddit}\"\n","    elif username != \"\" and subreddit == \"\":\n","        filter_string = f\"author={username}\"\n","    else:\n","        filter_string = f\"author={username}&subreddit={subreddit}\"\n","        \n","    downloadFromUrl(\"submission\", filter_string)\n","    print(\" \"+subreddit+\" ********------------------COMPLETED----------********************************\")\n","    time.sleep(1.5)"],"metadata":{"id":"dcfsgA4yrs8s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.columns =['upvotes','Date','Post', 'sub']\n","subs =[]\n","for line in df['sub']:\n","  line = re.sub('subreddit=', '', line)\n","  subs.append(line)\n","df['sub'] = subs\n","df"],"metadata":{"id":"V7qW5HB3rs7e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.drop_duplicates(subset =[\"Post\"],\n","                     keep = 'first', inplace = True)\n","df"],"metadata":{"id":"kxPjtCBqs2iY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['sub'].value_counts()"],"metadata":{"id":"7cRLHbBor33s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.to_csv('EXEC_RedditTesla.csv', sep=',', index = False)\n","print('csv done')"],"metadata":{"id":"9s45EISTr58u"},"execution_count":null,"outputs":[]}],"metadata":{"interpreter":{"hash":"7cfaad4ee1f7b24a8301a306757e944b0a86f6dac4ed5469ec061c4fc3f8e632"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"Reddit BOT 1.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}